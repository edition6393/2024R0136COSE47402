{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jNOsgBEzKucv"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "from dassl.utils import setup_logger, set_random_seed, collect_env_info\n",
    "from dassl.config import get_cfg_default\n",
    "from dassl.engine import build_trainer\n",
    "\n",
    "# custom\n",
    "import datasets.oxford_pets\n",
    "import datasets.oxford_flowers\n",
    "import datasets.fgvc_aircraft\n",
    "import datasets.dtd\n",
    "import datasets.eurosat\n",
    "import datasets.stanford_cars\n",
    "import datasets.food101\n",
    "import datasets.sun397\n",
    "import datasets.caltech101\n",
    "import datasets.ucf101\n",
    "import datasets.imagenet\n",
    "\n",
    "import datasets.imagenet_sketch\n",
    "import datasets.imagenetv2\n",
    "import datasets.imagenet_a\n",
    "import datasets.imagenet_r\n",
    "\n",
    "def print_args(args, cfg):\n",
    "    print(\"***************\")\n",
    "    print(\"** Arguments **\")\n",
    "    print(\"***************\")\n",
    "    optkeys = list(args.__dict__.keys())\n",
    "    optkeys.sort()\n",
    "    for key in optkeys:\n",
    "        print(\"{}: {}\".format(key, args.__dict__[key]))\n",
    "    print(\"************\")\n",
    "    print(\"** Config **\")\n",
    "    print(\"************\")\n",
    "    print(cfg)\n",
    "\n",
    "\n",
    "def reset_cfg(cfg, args):\n",
    "    if args.root:\n",
    "        cfg.DATASET.ROOT = args.root\n",
    "\n",
    "    if args.output_dir:\n",
    "        cfg.OUTPUT_DIR = args.output_dir\n",
    "\n",
    "    if args.resume:\n",
    "        cfg.RESUME = args.resume\n",
    "\n",
    "    if args.seed:\n",
    "        cfg.SEED = args.seed\n",
    "\n",
    "    if args.source_domains:\n",
    "        cfg.DATASET.SOURCE_DOMAINS = args.source_domains\n",
    "\n",
    "    if args.target_domains:\n",
    "        cfg.DATASET.TARGET_DOMAINS = args.target_domains\n",
    "\n",
    "    if args.transforms:\n",
    "        cfg.INPUT.TRANSFORMS = args.transforms\n",
    "\n",
    "    if args.trainer:\n",
    "        cfg.TRAINER.NAME = args.trainer\n",
    "\n",
    "    if args.backbone:\n",
    "        cfg.MODEL.BACKBONE.NAME = args.backbone\n",
    "\n",
    "    if args.head:\n",
    "        cfg.MODEL.HEAD.NAME = args.head\n",
    "\n",
    "    cfg.DATASET.SUBSAMPLE_CLASSES = args.subsample_classes\n",
    "    cfg.DATALOADER.TRAIN_X.BATCH_SIZE = args.train_batch_size\n",
    "    cfg.OPTIM.MAX_EPOCH = args.epoch\n",
    "\n",
    "\n",
    "def extend_cfg(cfg):\n",
    "    \"\"\"\n",
    "    Add new config variables.\n",
    "\n",
    "    E.g.\n",
    "        from yacs.config import CfgNode as CN\n",
    "        cfg.TRAINER.MY_MODEL = CN()\n",
    "        cfg.TRAINER.MY_MODEL.PARAM_A = 1.\n",
    "        cfg.TRAINER.MY_MODEL.PARAM_B = 0.5\n",
    "        cfg.TRAINER.MY_MODEL.PARAM_C = False\n",
    "    \"\"\"\n",
    "    from yacs.config import CfgNode as CN\n",
    "    \n",
    "    cfg.VERBOSE = True\n",
    "    cfg.TRAINER.MYTEMP = CN()\n",
    "    cfg.TRAINER.MYTEMP.K = 8\n",
    "    cfg.TRAINER.MYTEMP.CTX_INIT = ''\n",
    "    cfg.TRAINER.MYTEMP.PREC = 'fp16'\n",
    "\n",
    "    cfg.TRAINER.COCOOP = CN()\n",
    "    cfg.TRAINER.COCOOP.N_CTX = 4\n",
    "    cfg.TRAINER.COCOOP.CTX_INIT = 'a photo of a'\n",
    "    cfg.TRAINER.COCOOP.PREC = 'fp16'\n",
    "\n",
    "    cfg.TRAINER.COOP = CN()\n",
    "    cfg.TRAINER.COOP.N_CTX = 4\n",
    "    cfg.TRAINER.COOP.CSC = False\n",
    "    cfg.TRAINER.COOP.CLASS_TOKEN_POSITION = ''\n",
    "    cfg.TRAINER.COOP.PREC = 'fp16'\n",
    "    cfg.TRAINER.COOP.CTX_INIT = ''\n",
    "\n",
    "    cfg.TRAINER.LP = CN()\n",
    "    cfg.TRAINER.LP.PREC = 'fp16'\n",
    "    cfg.TRAINER.LP.PROMPT = 'A photo of a {cls_name}'\n",
    "\n",
    "\n",
    "\n",
    "    cfg.DATASET.SUBSAMPLE_CLASSES = \"all\"  # all, base or new\n",
    "    cfg.DATASET.PROMPT = \"a photo of a _.\"\n",
    "    cfg.DATASET.NUM_SHOTS = 16\n",
    "    \n",
    "def setup_cfg(args):\n",
    "    cfg = get_cfg_default()\n",
    "    extend_cfg(cfg)\n",
    "\n",
    "    # 1. From the dataset config file\n",
    "    if args.dataset_config_file:\n",
    "        cfg.merge_from_file(args.dataset_config_file)\n",
    "\n",
    "    # 2. From the method config file\n",
    "    if args.config_file:\n",
    "        cfg.merge_from_file(args.config_file)\n",
    "\n",
    "    # 3. From input arguments\n",
    "    reset_cfg(cfg, args)\n",
    "\n",
    "    # 4. From optional input arguments\n",
    "    cfg.merge_from_list(args.opts)\n",
    "\n",
    "    cfg.freeze()\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    cfg = setup_cfg(args)\n",
    "    set_random_seed(2)\n",
    "        \n",
    "    setup_logger(cfg.OUTPUT_DIR)\n",
    "\n",
    "    if torch.cuda.is_available() and cfg.USE_CUDA:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        random.seed(2)\n",
    "        np.random.seed(2)\n",
    "\n",
    "    print_args(args, cfg)\n",
    "    print(\"Collecting env info ...\")\n",
    "    print(\"** System info **\\n{}\\n\".format(collect_env_info()))\n",
    "\n",
    "    trainer = build_trainer(cfg)\n",
    "\n",
    "    if args.eval_only:\n",
    "        trainer.load_model(args.model_dir, epoch=args.load_epoch)\n",
    "        trainer.test()\n",
    "        return\n",
    "\n",
    "    #if not args.no_train:\n",
    "    trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MYTEMP\n",
    "\n",
    "import os.path as osp\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from dassl.engine import TRAINER_REGISTRY, TrainerX\n",
    "from dassl.metrics import compute_accuracy\n",
    "from dassl.utils import load_pretrained_weights, load_checkpoint\n",
    "from dassl.optim import build_optimizer, build_lr_scheduler\n",
    "\n",
    "from clip import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "\n",
    "def load_clip_to_cpu(cfg):\n",
    "    backbone_name = cfg.MODEL.BACKBONE.NAME\n",
    "    url = clip._MODELS[backbone_name]\n",
    "    model_path = clip._download(url)\n",
    "\n",
    "    try:\n",
    "        # loading JIT archive\n",
    "        model = torch.jit.load(model_path, map_location=\"cpu\").eval()\n",
    "        state_dict = None\n",
    "\n",
    "    except RuntimeError:\n",
    "        state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "\n",
    "    if cfg.TRAINER.NAME == \"\":\n",
    "      design_trainer = \"CoOp\"\n",
    "    else:\n",
    "      design_trainer = cfg.TRAINER.NAME\n",
    "    design_details = {\"trainer\": design_trainer,\n",
    "                      \"vision_depth\": 0,\n",
    "                      \"language_depth\": 0, \"vision_ctx\": 0,\n",
    "                      \"language_ctx\": 0}\n",
    "    model = clip.build_model(state_dict or model.state_dict())\n",
    "\n",
    "    return model\n",
    "\n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self, cfg, clip_model):\n",
    "        super().__init__()\n",
    "        positional_embedding = clip_model.positional_embedding\n",
    "\n",
    "        # Make sure K >= 1\n",
    "        assert cfg.TRAINER.MYTEMP.K >= 2, \"K should be bigger than 1\"\n",
    "\n",
    "        self.K = cfg.TRAINER.MYTEMP.K # the number of prompt pair\n",
    "        self.n_ctx = self.K\n",
    "        self.dtype = clip_model.dtype\n",
    "        self.d_t = clip_model.ln_final.weight.shape[0] #512\n",
    "        self.d_v = 768\n",
    "\n",
    "        clip_imsize = clip_model.visual.input_resolution # 224\n",
    "        cfg_imsize = cfg.INPUT.SIZE[0] # (224, 224)[0]\n",
    "        assert cfg_imsize == clip_imsize, f\"cfg_imsize ({cfg_imsize}) must equal to clip_imsize ({clip_imsize})\"\n",
    "\n",
    "        self.initialization_token(clip_model)\n",
    "        \n",
    "    def initialization_token(self, clip_model):\n",
    "        #### text token initialization #####\n",
    "        \n",
    "        text_token = clip_model.token_embedding(torch.tensor([49407]))\n",
    "        text_token = text_token.repeat(self.K, 1)\n",
    "        text_noise = torch.randn(self.K, self.d_t)\n",
    "        text_noise = text_noise / text_noise.norm(dim=-1, keepdim=True)\n",
    "        text_token += 0.1 * text_noise\n",
    "        text_token = text_token.type(self.dtype)\n",
    "        self.text_prompt = nn.Parameter(text_token)\n",
    "        '''\n",
    "        t_prompt_vec = torch.empty(self.K, self.d_t, dtype=self.dtype)\n",
    "        nn.init.normal_(t_prompt_vec, std=0.02)\n",
    "        self.text_prompt = nn.Parameter(t_prompt_vec, requires_grad=True)\n",
    "        '''\n",
    "        #### visual token initialization ####\n",
    "        \n",
    "        visual_token = clip_model.visual.class_embedding\n",
    "        visual_token = visual_token.repeat(self.K, 1)\n",
    "        visual_noise = torch.randn(self.K, self.d_v)\n",
    "        visual_noise = visual_noise / visual_noise.norm(dim=-1, keepdim=True)\n",
    "        visual_token += 0.1 * visual_noise\n",
    "        visual_token = visual_token.type(self.dtype)\n",
    "        self.img_prompt = nn.Parameter(visual_token)\n",
    "        '''\n",
    "        v_prompt_vec = torch.empty(self.K, self.d_v, dtype=self.dtype)\n",
    "        nn.init.normal_(v_prompt_vec, std=0.02)\n",
    "        self.img_prompt = nn.Parameter(v_prompt_vec, requires_grad=True)\n",
    "        '''\n",
    "    def forward(self):\n",
    "        return self.text_prompt, self.img_prompt\n",
    "\n",
    "\n",
    "class CustomCLIP(nn.Module):\n",
    "    '''\n",
    "    cfg : model parameters\n",
    "    device : model device\n",
    "    layer : # of query generate FFN layers\n",
    "    '''\n",
    "    def __init__(self, cfg, classnames, prompt, clipmodel):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # text encoder\n",
    "        self.token_embedding = clipmodel.token_embedding\n",
    "        self.text_pos_embedding = clipmodel.positional_embedding\n",
    "        self.text_transformers = clipmodel.transformer\n",
    "        self.text_ln_final = clipmodel.ln_final\n",
    "        self.text_proj = clipmodel.text_projection\n",
    "\n",
    "        # vision encoder\n",
    "        self.img_patch_embedding = clipmodel.visual.conv1\n",
    "        self.img_cls_embedding = clipmodel.visual.class_embedding\n",
    "        self.img_pos_embedding = clipmodel.visual.positional_embedding\n",
    "        self.img_pre_ln = clipmodel.visual.ln_pre\n",
    "        self.img_transformer = clipmodel.visual.transformer\n",
    "        self.img_post_ln = clipmodel.visual.ln_post\n",
    "        self.img_proj = clipmodel.visual.proj\n",
    "\n",
    "        # logit\n",
    "        self.logit_scale = clipmodel.logit_scale\n",
    "        \n",
    "        # initialization token\n",
    "        self.prompt_learner = PromptLearner(self.cfg, clipmodel)\n",
    "\n",
    "        #\n",
    "        self.dtype = clipmodel.dtype\n",
    "        self.prompts = self.make_prompts(classnames, prompt) # [\"a photo of a dog.\", \"..\"]\n",
    "\n",
    "        # define mask\n",
    "        self.define_mask()\n",
    "\n",
    "    def make_prompts(self, classnames, prompt):\n",
    "        prompts = [prompt.replace('_', c) for c in classnames]\n",
    "        with torch.no_grad():\n",
    "            self.text_tokenized = torch.cat([clip.tokenize(p) for p in prompts])\n",
    "            self.text_x = self.token_embedding(self.text_tokenized).type(self.dtype) + self.text_pos_embedding.type(self.dtype)\n",
    "            self.len_prompts = self.text_tokenized.argmax(dim=-1) + 1\n",
    "        return prompts\n",
    "\n",
    "    def define_mask(self):\n",
    "        len_max = 77\n",
    "        attn_head = 8\n",
    "\n",
    "        text_mask = torch.empty(0, len_max, len_max)\n",
    "        for idx in self.len_prompts:\n",
    "            mask = torch.empty(len_max, len_max)\n",
    "            mask.fill_(float(\"-inf\"))\n",
    "            mask.triu_(1)  # zero out the lower diagonal\n",
    "            mask[:, idx:].fill_(float(\"-inf\"))\n",
    "            text_mask = torch.cat([text_mask, mask.repeat(attn_head, 1, 1)])\n",
    "        self.text_mask = text_mask\n",
    "\n",
    "        # image encoder mask\n",
    "        att_size = 1 + 14 * 14 + self.cfg.TRAINER.MYTEMP.K\n",
    "        visual_mask = torch.zeros((att_size, att_size), dtype=self.dtype, requires_grad=False)\n",
    "        visual_mask[:, -1 * self.cfg.TRAINER.MYTEMP.K:] = float(\"-inf\")\n",
    "        #####\n",
    "\n",
    "        self.visual_mask = visual_mask\n",
    "\n",
    "    def forward(self, image, label=None):\n",
    "        device = image.device\n",
    "\n",
    "        # load mask from predefined masks\n",
    "        text_mask = self.text_mask\n",
    "        visual_mask = self.visual_mask\n",
    "        K = self.cfg.TRAINER.MYTEMP.K\n",
    "\n",
    "        # load prompts from prompt learner\n",
    "        text_prompt, image_prompt = self.prompt_learner()\n",
    "\n",
    "        ####################### text ###########################        \n",
    "        text_x = self.text_x\n",
    "        text_x = text_x.to(device)\n",
    "        \n",
    "        for i in range(K):\n",
    "            text_x[torch.arange(text_x.shape[0]), self.len_prompts+i, :] = text_prompt[i, :].repeat(text_x.shape[0], 1)\n",
    "        \n",
    "\n",
    "        text_x = text_x.permute(1, 0, 2)  # NLD -> LND\n",
    "        text_x = self.text_transformers(text_x, text_mask)\n",
    "        text_x = text_x.permute(1, 0, 2)\n",
    "        text_x = self.text_ln_final(text_x).type(self.dtype)\n",
    "\n",
    "        text_f = torch.empty(text_x.shape[0], 0, 512, device=device, dtype=self.dtype)\n",
    "        for i in range(K):\n",
    "            idx = self.len_prompts + i\n",
    "            x = text_x[torch.arange(text_x.shape[0]), idx]\n",
    "            text_f = torch.cat([text_f, x[:, None, :]], dim=1)\n",
    "\n",
    "        text_f = text_f @ self.text_proj\n",
    "        t_f = text_x[torch.arange(text_x.shape[0]), self.text_tokenized.argmax(dim=-1)] @ self.text_proj\n",
    "        \n",
    "        ####################### img ###########################\n",
    "        batch_size = image.shape[0]\n",
    "        \n",
    "        # forward propagate image features with token concatenation\n",
    "        image_embedding = self.img_patch_embedding(image.type(self.dtype)) # (batch_size, h_dim, 7, 7)\n",
    "        image_embedding = image_embedding.reshape(batch_size, image_embedding.shape[1], -1)\n",
    "        image_embedding = image_embedding.permute(0,2,1) # (batch_size, 49, h_dim)\n",
    "        image_embedding = torch.cat([self.img_cls_embedding.repeat(batch_size,1,1).type(self.dtype), image_embedding], dim=1) # 16 (batch_size, 50, h_dim)\n",
    "        img_x = image_embedding + self.img_pos_embedding.type(self.dtype) # (N,L,D)\n",
    "        # concatenation the token on visual encoder\n",
    "        img_x = torch.cat([img_x, image_prompt.repeat(batch_size, 1, 1)], dim=1)\n",
    "        # image encoder\n",
    "        img_x = self.img_pre_ln(img_x)\n",
    "        img_x = img_x.permute(1, 0, 2)\n",
    "        img_x = self.img_transformer(img_x, visual_mask)\n",
    "        img_x = img_x.permute(1, 0, 2)\n",
    "        img_f = self.img_post_ln(img_x[:, -1 * K:, :]) @ self.img_proj\n",
    "        i_f = self.img_post_ln(img_x[:, 0, :]) @ self.img_proj\n",
    "        ####################### logit ###########################\n",
    "        # logit\n",
    "\n",
    "        text_f = text_f / text_f.norm(dim=-1, keepdim=True)\n",
    "        t_f = t_f / t_f.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        img_f = img_f / img_f.norm(dim=-1, keepdim=True)\n",
    "        i_f = i_f / i_f.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logits = torch.zeros(img_f.shape[0], text_f.shape[0], device=device)\n",
    "        for i in range(K):\n",
    "            i_img_f = img_f[:,i,:]\n",
    "            i_text_f = text_f[:,i,:]\n",
    "            logit = self.logit_scale.exp() * i_img_f @ i_text_f.t()\n",
    "            logits += logit\n",
    "        logits /= K\n",
    "\n",
    "        if self.prompt_learner.training:\n",
    "            return F.cross_entropy(logits, label)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "@TRAINER_REGISTRY.register()\n",
    "class MYTEMP(TrainerX):\n",
    "    def check_cfg(self, cfg):\n",
    "        assert cfg.TRAINER.MYTEMP.PREC in [\"fp16\", \"fp32\", \"amp\"]\n",
    "\n",
    "    def build_model(self):\n",
    "        cfg = self.cfg\n",
    "        classnames = self.dm.dataset.classnames\n",
    "\n",
    "        print(f\"Loading CLIP (backbone: {cfg.MODEL.BACKBONE.NAME})\")\n",
    "        clip_model = load_clip_to_cpu(cfg)\n",
    "        \n",
    "        if cfg.TRAINER.MYTEMP.PREC == \"fp32\" or cfg.TRAINER.MYTEMP.PREC == \"amp\":\n",
    "            # CLIP's default precision is fp16\n",
    "            clip_model.float()\n",
    "\n",
    "        prompt = cfg.DATASET.PROMPT\n",
    "        ############################################# 통일 #####\n",
    "\n",
    "        print(\"Building custom CLIP\")\n",
    "        self.model = CustomCLIP(cfg, classnames, prompt, clip_model)\n",
    "        \n",
    "        # parameter freeze\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"prompt_learner\" not in name:\n",
    "                param.requires_grad_(False)\n",
    "        \n",
    "        # Double check\n",
    "        enabled = set()\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                enabled.add(name)\n",
    "        print(f\"Parameters to be updated: {enabled}\")\n",
    "\n",
    "        if cfg.MODEL.INIT_WEIGHTS:\n",
    "            load_pretrained_weights(self.model.prompt_learner, cfg.MODEL.INIT_WEIGHTS)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        # NOTE: only give prompt_learner to the optimizer\n",
    "        self.optim = build_optimizer(self.model.prompt_learner, cfg.OPTIM)\n",
    "        self.sched = build_lr_scheduler(self.optim, cfg.OPTIM)\n",
    "        self.register_model(\"prompt_learner\", self.model.prompt_learner, self.optim, self.sched)\n",
    "\n",
    "        self.scaler = GradScaler() if cfg.TRAINER.MYTEMP.PREC == \"amp\" else None\n",
    "\n",
    "        # Note that multi-gpu training could be slow because CLIP's size is\n",
    "        # big, which slows down the copy operation in DataParallel\n",
    "        device_count = torch.cuda.device_count()\n",
    "        if device_count > 1:\n",
    "            print(f\"Multiple GPUs detected (n_gpus={device_count}), use all of them!\")\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "\n",
    "        # nan detector\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    def forward_backward(self, batch):\n",
    "        image, label = self.parse_batch_train(batch)\n",
    "\n",
    "        model = self.model\n",
    "        optim = self.optim\n",
    "        scaler = self.scaler\n",
    "\n",
    "        prec = self.cfg.TRAINER.MYTEMP.PREC\n",
    "        if prec == \"amp\":\n",
    "            with autocast():\n",
    "                loss = model(image, label)\n",
    "            optim.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optim)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss = model(image, label)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        loss_summary = {\"loss\": loss.item()}\n",
    "\n",
    "        if (self.batch_idx + 1) == self.num_batches:\n",
    "            self.update_lr()\n",
    "\n",
    "        return loss_summary\n",
    "\n",
    "    def parse_batch_train(self, batch):\n",
    "        input = batch[\"img\"]\n",
    "        label = batch[\"label\"]\n",
    "        input = input.to(self.device)\n",
    "        label = label.to(self.device)\n",
    "        return input, label\n",
    "\n",
    "    def load_model(self, directory, epoch=None):\n",
    "        if not directory:\n",
    "            print(\"Note that load_model() is skipped as no pretrained model is given\")\n",
    "            return\n",
    "\n",
    "        names = self.get_model_names()\n",
    "\n",
    "        # By default, the best model is loaded\n",
    "        model_file = \"model-best.pth.tar\"\n",
    "\n",
    "        if epoch is not None:\n",
    "            model_file = \"model.pth.tar-\" + str(epoch)\n",
    "\n",
    "        for name in names:\n",
    "            model_path = osp.join(directory, name, model_file)\n",
    "\n",
    "            if not osp.exists(model_path):\n",
    "                raise FileNotFoundError('Model not found at \"{}\"'.format(model_path))\n",
    "\n",
    "            checkpoint = load_checkpoint(model_path)\n",
    "            state_dict = checkpoint[\"state_dict\"]\n",
    "            epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "            # Ignore fixed token vectors\n",
    "            if \"token_prefix\" in state_dict:\n",
    "                del state_dict[\"token_prefix\"]\n",
    "\n",
    "            if \"token_suffix\" in state_dict:\n",
    "                del state_dict[\"token_suffix\"]\n",
    "\n",
    "            print(\"Loading weights to {} \" 'from \"{}\" (epoch = {})'.format(name, model_path, epoch))\n",
    "            # set strict=False\n",
    "            self._models[name].load_state_dict(state_dict, strict=False)\n",
    "\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2CGZlqo-HtRN"
   },
   "source": [
    "### **Q2. Trainining CoCoOp**\n",
    "\n",
    "In this task, you will train CoCoOp on the EuroSAT dataset. If your implementation of CoCoOp in Question 1 is correct, the following code should execute without errors. Please submit the execution file so we can evaluate whether your code runs without any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Zy3bAMnBMrXP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/TEMP/main.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "epoch: 20\n",
      "eval_only: False\n",
      "head: \n",
      "model_dir: \n",
      "opts: []\n",
      "output_dir: outputs/mytemp\n",
      "resume: \n",
      "root: data/\n",
      "seed: 2\n",
      "source_domains: None\n",
      "subsample_classes: base\n",
      "target_domains: None\n",
      "train_batch_size: 4\n",
      "trainer: MYTEMP\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  PROMPT: a photo of a _.\n",
      "  ROOT: data/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: base\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.01\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 20\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: outputs/mytemp\n",
      "RESUME: \n",
      "SEED: 2\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX: 4\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: \n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 4\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  LP:\n",
      "    PREC: fp16\n",
      "    PROMPT: A photo of a {cls_name}\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  MYTEMP:\n",
      "    CTX_INIT: a photo of a\n",
      "    K: 4\n",
      "    PREC: fp16\n",
      "  NAME: MYTEMP\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.4.1\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.4\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.5 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: Could not collect\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8.20 (default, Oct  3 2024, 15:24:27)  [GCC 11.2.0] (64-bit runtime)\n",
      "Python platform: Linux-6.8.0-49-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.4.99\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4070 SUPER\n",
      "Nvidia driver version: 550.120\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        43 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               12\n",
      "On-line CPU(s) list:                  0-11\n",
      "Vendor ID:                            AuthenticAMD\n",
      "Model name:                           AMD Ryzen 5 2600X Six-Core Processor\n",
      "CPU family:                           23\n",
      "Model:                                8\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   6\n",
      "Socket(s):                            1\n",
      "Stepping:                             2\n",
      "Frequency boost:                      enabled\n",
      "CPU max MHz:                          3600.0000\n",
      "CPU min MHz:                          2200.0000\n",
      "BogoMIPS:                             7199.73\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sev sev_es\n",
      "Virtualization:                       AMD-V\n",
      "L1d cache:                            192 KiB (6 instances)\n",
      "L1i cache:                            384 KiB (6 instances)\n",
      "L2 cache:                             3 MiB (6 instances)\n",
      "L3 cache:                             16 MiB (2 instances)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0-11\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Not affected\n",
      "Vulnerability Mds:                    Not affected\n",
      "Vulnerability Meltdown:               Not affected\n",
      "Vulnerability Mmio stale data:        Not affected\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Mitigation; untrained return thunk; SMT vulnerable\n",
      "Vulnerability Spec rstack overflow:   Mitigation; Safe RET\n",
      "Vulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\n",
      "Vulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
      "Vulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Not affected\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.3\n",
      "[pip3] torch==2.4.1\n",
      "[pip3] torchaudio==2.4.1\n",
      "[pip3] torchvision==0.20.0\n",
      "[pip3] triton==3.0.0\n",
      "[conda] blas                      1.0                         mkl  \n",
      "[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\n",
      "[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\n",
      "[conda] mkl                       2023.1.0         h213fc3f_46344  \n",
      "[conda] mkl-service               2.4.0            py38h5eee18b_1  \n",
      "[conda] mkl_fft                   1.3.8            py38h5eee18b_0  \n",
      "[conda] mkl_random                1.2.4            py38hdb19cb5_0  \n",
      "[conda] numpy                     1.24.3           py38hf6e8229_1  \n",
      "[conda] numpy-base                1.24.3           py38h060ed82_1  \n",
      "[conda] pytorch                   2.4.1           py3.8_cuda12.4_cudnn9.1.0_0    pytorch\n",
      "[conda] pytorch-cuda              12.4                 hc786d27_7    pytorch\n",
      "[conda] pytorch-mutex             1.0                        cuda    pytorch\n",
      "[conda] torchaudio                2.4.1                py38_cu124    pytorch\n",
      "[conda] torchtriton               3.0.0                      py38    pytorch\n",
      "[conda] torchvision               0.20.0               py38_cu124    pytorch\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: MYTEMP\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /home/john/finalproject/data/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /home/john/finalproject/data/eurosat/split_fewshot/shot_16-seed_2.pkl\n",
      "SUBSAMPLE BASE CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  5\n",
      "# train_x  80\n",
      "# val      20\n",
      "# test     4,200\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/anaconda3/envs/dassl/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/home/john/finalproject/dassl/utils/torchtools.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fpath, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building custom CLIP\n",
      "Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}\n",
      "Loading evaluator: Classification\n",
      "Found checkpoint at outputs/mytemp (will resume training)\n",
      "Loading checkpoint from \"outputs/mytemp/prompt_learner/model.pth.tar-20\"\n",
      "Loaded model weights\n",
      "Loaded optimizer\n",
      "Loaded scheduler\n",
      "Previous epoch: 20\n",
      "Initialize tensorboard (log_dir=outputs/mytemp/tensorboard)\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:07<00:00,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> result\n",
      "* total: 4,200\n",
      "* correct: 3,929\n",
      "* accuracy: 93.5%\n",
      "* error: 6.5%\n",
      "* macro_f1: 93.6%\n",
      "Elapsed: 0:00:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--root\", type=str, default=\"data/\", help=\"path to dataset\")\n",
    "parser.add_argument(\"--output-dir\", type=str, default=\"\", help=\"output directory\")\n",
    "parser.add_argument(\n",
    "    \"--resume\",\n",
    "    type=str,\n",
    "    default=\"\",\n",
    "    help=\"checkpoint directory (from which the training resumes)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\", type=int, default=2, help=\"only positive value enables a fixed seed\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--source-domains\", type=str, nargs=\"+\", help=\"source domains for DA/DG\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--target-domains\", type=str, nargs=\"+\", help=\"target domains for DA/DG\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--transforms\", type=str, nargs=\"+\", help=\"data augmentation methods\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\"--trainer\", type=str, default=\"\", help=\"name of trainer\")\n",
    "parser.add_argument(\"--backbone\", type=str, default=\"\", help=\"name of CNN backbone\")\n",
    "parser.add_argument(\"--head\", type=str, default=\"\", help=\"name of head\")\n",
    "parser.add_argument(\"--eval-only\", action=\"store_true\", help=\"evaluation only\")\n",
    "parser.add_argument(\n",
    "    \"--model-dir\",\n",
    "    type=str,\n",
    "    default=\"\",\n",
    "    help=\"load model from this directory for eval-only mode\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"opts\",\n",
    "    default=None,\n",
    "    nargs=argparse.REMAINDER,\n",
    "    help=\"modify config options using the command-line\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset-config-file\",\n",
    "    type=str,\n",
    "    default=\"configs/datasets/eurosat.yaml\",\n",
    "    help=\"path to config file for dataset setup\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--config-file\", type=str, default=\"configs/trainers/TEMP/main.yaml\", help=\"path to config file\"\n",
    ")\n",
    "\n",
    "# parser.add_argument(\"--subsample-classes\", type=str, default=\"base\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "args.trainer = \"MYTEMP\"\n",
    "args.train_batch_size = 4\n",
    "args.epoch = 20\n",
    "args.output_dir = \"outputs/mytemp\"\n",
    "\n",
    "args.subsample_classes = \"base\"\n",
    "args.eval_only = False\n",
    "mytemp_base_acc = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Xql7WpJ5vPII"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/TEMP/main.yaml\n",
      "dataset_config_file: configs/datasets/eurosat.yaml\n",
      "epoch: 20\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 20\n",
      "model_dir: outputs/mytemp\n",
      "opts: []\n",
      "output_dir: outputs/mytemp/new_classes\n",
      "resume: \n",
      "root: data/\n",
      "seed: 2\n",
      "source_domains: None\n",
      "subsample_classes: new\n",
      "target_domains: None\n",
      "train_batch_size: 4\n",
      "trainer: MYTEMP\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: EuroSAT\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  PROMPT: a photo of a _.\n",
      "  ROOT: data/\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: new\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.01\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 20\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: outputs/mytemp/new_classes\n",
      "RESUME: \n",
      "SEED: 2\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  COCOOP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX: 4\n",
      "    PREC: fp16\n",
      "  COOP:\n",
      "    CLASS_TOKEN_POSITION: \n",
      "    CSC: False\n",
      "    CTX_INIT: \n",
      "    N_CTX: 4\n",
      "    PREC: fp16\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  LP:\n",
      "    PREC: fp16\n",
      "    PROMPT: A photo of a {cls_name}\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  MYTEMP:\n",
      "    CTX_INIT: a photo of a\n",
      "    K: 4\n",
      "    PREC: fp16\n",
      "  NAME: MYTEMP\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 2.4.1\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.4\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.5 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: Could not collect\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8.20 (default, Oct  3 2024, 15:24:27)  [GCC 11.2.0] (64-bit runtime)\n",
      "Python platform: Linux-6.8.0-49-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.4.99\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 4070 SUPER\n",
      "Nvidia driver version: 550.120\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                         x86_64\n",
      "CPU op-mode(s):                       32-bit, 64-bit\n",
      "Address sizes:                        43 bits physical, 48 bits virtual\n",
      "Byte Order:                           Little Endian\n",
      "CPU(s):                               12\n",
      "On-line CPU(s) list:                  0-11\n",
      "Vendor ID:                            AuthenticAMD\n",
      "Model name:                           AMD Ryzen 5 2600X Six-Core Processor\n",
      "CPU family:                           23\n",
      "Model:                                8\n",
      "Thread(s) per core:                   2\n",
      "Core(s) per socket:                   6\n",
      "Socket(s):                            1\n",
      "Stepping:                             2\n",
      "Frequency boost:                      enabled\n",
      "CPU max MHz:                          3600.0000\n",
      "CPU min MHz:                          2200.0000\n",
      "BogoMIPS:                             7199.73\n",
      "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sev sev_es\n",
      "Virtualization:                       AMD-V\n",
      "L1d cache:                            192 KiB (6 instances)\n",
      "L1i cache:                            384 KiB (6 instances)\n",
      "L2 cache:                             3 MiB (6 instances)\n",
      "L3 cache:                             16 MiB (2 instances)\n",
      "NUMA node(s):                         1\n",
      "NUMA node0 CPU(s):                    0-11\n",
      "Vulnerability Gather data sampling:   Not affected\n",
      "Vulnerability Itlb multihit:          Not affected\n",
      "Vulnerability L1tf:                   Not affected\n",
      "Vulnerability Mds:                    Not affected\n",
      "Vulnerability Meltdown:               Not affected\n",
      "Vulnerability Mmio stale data:        Not affected\n",
      "Vulnerability Reg file data sampling: Not affected\n",
      "Vulnerability Retbleed:               Mitigation; untrained return thunk; SMT vulnerable\n",
      "Vulnerability Spec rstack overflow:   Mitigation; Safe RET\n",
      "Vulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\n",
      "Vulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
      "Vulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\n",
      "Vulnerability Srbds:                  Not affected\n",
      "Vulnerability Tsx async abort:        Not affected\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] flake8==3.7.9\n",
      "[pip3] numpy==1.24.3\n",
      "[pip3] torch==2.4.1\n",
      "[pip3] torchaudio==2.4.1\n",
      "[pip3] torchvision==0.20.0\n",
      "[pip3] triton==3.0.0\n",
      "[conda] blas                      1.0                         mkl  \n",
      "[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\n",
      "[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\n",
      "[conda] mkl                       2023.1.0         h213fc3f_46344  \n",
      "[conda] mkl-service               2.4.0            py38h5eee18b_1  \n",
      "[conda] mkl_fft                   1.3.8            py38h5eee18b_0  \n",
      "[conda] mkl_random                1.2.4            py38hdb19cb5_0  \n",
      "[conda] numpy                     1.24.3           py38hf6e8229_1  \n",
      "[conda] numpy-base                1.24.3           py38h060ed82_1  \n",
      "[conda] pytorch                   2.4.1           py3.8_cuda12.4_cudnn9.1.0_0    pytorch\n",
      "[conda] pytorch-cuda              12.4                 hc786d27_7    pytorch\n",
      "[conda] pytorch-mutex             1.0                        cuda    pytorch\n",
      "[conda] torchaudio                2.4.1                py38_cu124    pytorch\n",
      "[conda] torchtriton               3.0.0                      py38    pytorch\n",
      "[conda] torchvision               0.20.0               py38_cu124    pytorch\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: MYTEMP\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from /home/john/finalproject/data/eurosat/split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from /home/john/finalproject/data/eurosat/split_fewshot/shot_16-seed_2.pkl\n",
      "SUBSAMPLE NEW CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  5\n",
      "# train_x  80\n",
      "# val      20\n",
      "# test     3,900\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Parameters to be updated: {'prompt_learner.text_prompt', 'prompt_learner.img_prompt'}\n",
      "Loading evaluator: Classification\n",
      "Loading weights to prompt_learner from \"outputs/mytemp/prompt_learner/model.pth.tar-20\" (epoch = 20)\n",
      "Evaluate on the *test* set\n",
      "=> result\n",
      "* total: 3,900\n",
      "* correct: 2,302\n",
      "* accuracy: 59.0%\n",
      "* error: 41.0%\n",
      "* macro_f1: 55.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/anaconda3/envs/dassl/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/home/john/finalproject/dassl/utils/torchtools.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fpath, map_location=map_location)\n",
      "100%|██████████| 39/39 [00:06<00:00,  5.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Accuracy on the New Classes.\n",
    "args.model_dir = \"outputs/mytemp\"\n",
    "args.output_dir = \"outputs/mytemp/new_classes\"\n",
    "args.subsample_classes = \"new\"\n",
    "args.load_epoch = 20\n",
    "args.eval_only = True\n",
    "mytemp_novel_acc = main(args)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "G3n9blo4JO7m",
    "2CGZlqo-HtRN"
   ],
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dassl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
